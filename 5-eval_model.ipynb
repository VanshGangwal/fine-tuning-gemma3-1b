{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f773a076-2a57-4096-8dba-71231f8f25e3",
   "metadata": {},
   "source": [
    "# Evaluate Fine-tuned Model\n",
    "\n",
    "Code authored by: Shaw Talebi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa42de5c-6819-47c0-9c71-5df88ba09fa7",
   "metadata": {},
   "source": [
    "### imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a2126e55-02d1-4397-9741-41b11f4c060b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "from peft import PeftModel\n",
    "from utils.tool_calling import parse_tool_call, call_tool\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bc86ff2-f1aa-4c7a-b0ad-a3c32c07bacd",
   "metadata": {},
   "source": [
    "### load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fbbb9055-d960-40a1-ab4d-cfc429f4028f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dataset\n",
    "ds = load_dataset(\"shawhin/tool-use-finetuning\")\n",
    "ds_test = ds['test']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41344cac-d573-4cb1-8d0a-074acd93189f",
   "metadata": {},
   "source": [
    "### load models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f306fed0-3b4d-4829-9ab2-5c16db874a4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load base model\n",
    "model_name = \"google/gemma-3-1b-it\"\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    device_map=\"mps\",\n",
    ")\n",
    "\n",
    "# load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1658c1f0-67c7-4034-865d-e4e0ddec266c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load fine-tuned model\n",
    "finetuned_model_name = \"shawhin/gemma-3-1b-tool-use\"\n",
    "\n",
    "finetuned_model = AutoModelForCausalLM.from_pretrained(\n",
    "    finetuned_model_name,\n",
    "    device_map=\"mps\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d50f744c-9d0d-4d11-89eb-1cfd8bef404c",
   "metadata": {},
   "source": [
    "### generate responses using models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b1f19c67-84da-4bc3-bde7-7d480df6914d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model_tool_calling(generator, row):\n",
    "    \"\"\"\n",
    "    Evaluates whether the model correctly identifies the need for a tool,\n",
    "    calls the correct tool, and executes it successfully.\n",
    "    \"\"\"\n",
    "    # Extract values from row\n",
    "    messages = row['trace']\n",
    "    expected_tool_name = row.get('tool_name', None)\n",
    "\n",
    "    # Auto-infer if a tool is needed\n",
    "    tool_needed = expected_tool_name is not None and str(expected_tool_name).lower() != 'nan'\n",
    "\n",
    "    # Set system role on first message\n",
    "    messages = messages.copy()\n",
    "    messages[0]['role'] = 'system'\n",
    "\n",
    "    # Generate assistant output\n",
    "    output = generator(messages[:2], return_full_text=True)[0]\n",
    "    response = output['generated_text'][-1]\n",
    "\n",
    "    # Initialize tracking flags\n",
    "    model_called_tool = False\n",
    "    model_tool_name = None\n",
    "    model_called_correct_tool = False\n",
    "    tool_call_success = False\n",
    "\n",
    "    # Check if model issued a tool call\n",
    "    if \"<tool_call>\" in response['content']:\n",
    "        model_called_tool = True\n",
    "        parsed_result = parse_tool_call(response['content'])\n",
    "\n",
    "        if parsed_result is not None:\n",
    "            tool_name, tool_args = parsed_result\n",
    "            model_tool_name = tool_name\n",
    "\n",
    "            model_called_correct_tool = (tool_name == expected_tool_name)\n",
    "\n",
    "            try:\n",
    "                result = call_tool(tool_name, tool_args)\n",
    "                tool_call_success = True\n",
    "            except:\n",
    "                tool_call_success = False\n",
    "        else:\n",
    "            # tool_call marker present but malformed\n",
    "            model_tool_name = None\n",
    "            model_called_correct_tool = False\n",
    "            tool_call_success = False\n",
    "\n",
    "    # Final evaluation: did the model call a tool when it was needed?\n",
    "    if tool_needed:\n",
    "        model_called_tool_when_needed = model_called_tool\n",
    "    else:\n",
    "        model_called_tool_when_needed = not model_called_tool\n",
    "\n",
    "    return {\n",
    "        'response': response,\n",
    "        'model_called_tool': model_called_tool,\n",
    "        'model_tool_name': model_tool_name,\n",
    "        'model_called_tool_when_needed': model_called_tool_when_needed,\n",
    "        'model_called_correct_tool': model_called_correct_tool,\n",
    "        'tool_call_success': tool_call_success,\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cf0cd6b5-d712-4d10-9709-e5b91cff61ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps\n",
      "Device set to use mps\n"
     ]
    }
   ],
   "source": [
    "# create pipelines\n",
    "base_generator = pipeline(\"text-generation\", model=model, tokenizer=tokenizer, temperature=0.1)\n",
    "finetuned_generator = pipeline(\"text-generation\", model=finetuned_model, tokenizer=tokenizer, temperature=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "829878cd-2512-42e3-8671-c4387d34320c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating row: 0\n",
      "Evaluating row: 1\n",
      "Evaluating row: 2\n",
      "Evaluating row: 3\n",
      "Evaluating row: 4\n",
      "Evaluating row: 5\n",
      "Evaluating row: 6\n",
      "Evaluating row: 7\n",
      "Evaluating row: 8\n",
      "Evaluating row: 9\n",
      "Evaluating row: 10\n",
      "Evaluating row: 11\n",
      "Evaluating row: 12\n",
      "Evaluating row: 13\n",
      "Evaluating row: 14\n",
      "Evaluating row: 15\n",
      "Evaluating row: 16\n",
      "Evaluating row: 17\n",
      "Evaluating row: 18\n",
      "Evaluating row: 19\n",
      "Evaluating row: 20\n",
      "Evaluating row: 21\n",
      "Evaluating row: 22\n",
      "Evaluating row: 23\n",
      "Evaluating row: 24\n",
      "Evaluating row: 25\n",
      "Evaluating row: 26\n",
      "Evaluating row: 27\n",
      "Evaluating row: 28\n",
      "Evaluating row: 29\n",
      "Evaluating row: 30\n",
      "Evaluating row: 31\n",
      "Evaluating row: 32\n",
      "Evaluating row: 33\n",
      "Evaluating row: 34\n",
      "Evaluating row: 35\n",
      "Evaluating row: 36\n",
      "Evaluating row: 37\n",
      "Evaluating row: 38\n",
      "Evaluating row: 39\n",
      "Evaluating row: 40\n",
      "Evaluating row: 41\n",
      "Evaluating row: 42\n",
      "Evaluating row: 43\n",
      "Evaluating row: 44\n",
      "Evaluating row: 45\n",
      "Evaluating row: 46\n",
      "Evaluating row: 47\n",
      "Evaluating row: 48\n",
      "Evaluating row: 49\n",
      "Evaluating row: 50\n",
      "Evaluating row: 51\n",
      "Evaluating row: 52\n",
      "Evaluating row: 53\n",
      "Evaluating row: 54\n",
      "Evaluating row: 55\n",
      "Evaluating row: 56\n",
      "Evaluating row: 57\n",
      "Evaluating row: 58\n",
      "Evaluating row: 59\n",
      "CPU times: user 16min, sys: 1min 58s, total: 17min 58s\n",
      "Wall time: 18min 4s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "results_data = []\n",
    "\n",
    "for i, row in enumerate(ds_test):\n",
    "    print(\"Evaluating row:\", i)\n",
    "    # Generate base model results\n",
    "    base_results = evaluate_model_tool_calling(base_generator, row)\n",
    "    result_row = {\n",
    "        'model_name': model_name,\n",
    "        'query': row['query'],\n",
    "        'query_type': row['query_type'],\n",
    "        'num_tools_available': row['num_tools_available'],\n",
    "        'expected_tool_name': row['tool_name'],\n",
    "        **base_results\n",
    "    }\n",
    "    results_data.append(result_row)\n",
    "\n",
    "    # Generate fine-tuned model results\n",
    "    finetuned_results = evaluate_model_tool_calling(finetuned_generator, row)\n",
    "    finetuned_result_row = {\n",
    "        'model_name': finetuned_model_name,\n",
    "        'query': row['query'],\n",
    "        'query_type': row['query_type'],\n",
    "        'num_tools_available': row['num_tools_available'],\n",
    "        'expected_tool_name': row['tool_name'],\n",
    "        **finetuned_results\n",
    "    }\n",
    "    results_data.append(finetuned_result_row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "064e35e0-cfcb-4692-915e-7d98d3ccedb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = pd.DataFrame(results_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5883f552-15c5-492f-9b65-cffcdc143f0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# write results to file\n",
    "results_df.to_csv('data/eval_results.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a139c46-7f22-4267-9d72-6e0d1b9176f2",
   "metadata": {},
   "source": [
    "### evaluate models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f769d0eb-542d-449e-9879-d4de49406f01",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def compare_model_performance(results_df):\n",
    "    \"\"\"\n",
    "    Compare model performance based on tool calling metrics.\n",
    "    \n",
    "    Parameters:\n",
    "    results_df (pd.DataFrame): DataFrame containing evaluation results with columns:\n",
    "        - model_name\n",
    "        - model_called_tool_when_needed\n",
    "        - model_called_correct_tool\n",
    "        - tool_call_success\n",
    "        - expected_tool_name\n",
    "    \n",
    "    Returns:\n",
    "    pd.DataFrame: Performance comparison with models as rows and metrics as columns\n",
    "    \"\"\"\n",
    "    \n",
    "    # Filter for rows where a tool call was needed (expected_tool_name is not None/NaN)\n",
    "    tool_needed_rows = results_df[results_df['expected_tool_name'].notna() & \n",
    "                                 (results_df['expected_tool_name'] != 'nan')]\n",
    "    \n",
    "    # Group by model name and calculate pass rates\n",
    "    performance_metrics = results_df.groupby('model_name').agg({\n",
    "        'model_called_tool_when_needed': 'mean',\n",
    "    }).round(4)\n",
    "    \n",
    "    # Calculate metrics only for rows where tool was needed\n",
    "    if len(tool_needed_rows) > 0:\n",
    "        tool_metrics = tool_needed_rows.groupby('model_name').agg({\n",
    "            'model_called_correct_tool': 'mean',\n",
    "            'tool_call_success': 'mean'\n",
    "        }).round(4)\n",
    "        \n",
    "        # Combine the metrics\n",
    "        performance_metrics = pd.concat([performance_metrics, tool_metrics], axis=1)\n",
    "    else:\n",
    "        # If no rows need tools, set these metrics to None\n",
    "        performance_metrics['model_called_correct_tool'] = None\n",
    "        performance_metrics['tool_call_success'] = None\n",
    "    \n",
    "    # Rename columns for clarity\n",
    "    performance_metrics.columns = [\n",
    "        'Tool Called When Needed (%)',\n",
    "        'Correct Tool Called (%)', \n",
    "        'Tool Call Success (%)'\n",
    "    ]\n",
    "    \n",
    "    # Convert to percentages\n",
    "    performance_metrics = performance_metrics * 100\n",
    "    \n",
    "    return performance_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6a0087b1-779c-4f4d-90df-8cb2a148d7b6",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def compare_model_performance_by_column(results_df, split_column):\n",
    "    \"\"\"\n",
    "    Compare model performance based on tool calling metrics, split by specified column.\n",
    "    \n",
    "    Parameters:\n",
    "    results_df (pd.DataFrame): DataFrame containing evaluation results with columns:\n",
    "        - model_name\n",
    "        - model_called_tool_when_needed\n",
    "        - model_called_correct_tool\n",
    "        - tool_call_success\n",
    "        - expected_tool_name\n",
    "        - split_column (the column to split analysis by)\n",
    "    split_column (str): Column name to split the analysis by (e.g., 'query_type', 'num_tools_available')\n",
    "    \n",
    "    Returns:\n",
    "    pd.DataFrame: Performance comparison with models and split_column values as rows and metrics as columns\n",
    "    \"\"\"\n",
    "    \n",
    "    # Filter for rows where a tool call was needed (expected_tool_name is not None/NaN)\n",
    "    tool_needed_rows = results_df[results_df['expected_tool_name'].notna() & \n",
    "                                 (results_df['expected_tool_name'] != 'nan')]\n",
    "    \n",
    "    # Group by model name and split column, calculate pass rates for all rows\n",
    "    performance_metrics = results_df.groupby(['model_name', split_column]).agg({\n",
    "        'model_called_tool_when_needed': 'mean',\n",
    "    }).round(4)\n",
    "    \n",
    "    # Calculate metrics only for rows where tool was needed\n",
    "    if len(tool_needed_rows) > 0:\n",
    "        tool_metrics = tool_needed_rows.groupby(['model_name', split_column]).agg({\n",
    "            'model_called_correct_tool': 'mean',\n",
    "            'tool_call_success': 'mean'\n",
    "        }).round(4)\n",
    "        \n",
    "        # Combine the metrics\n",
    "        performance_metrics = pd.concat([performance_metrics, tool_metrics], axis=1)\n",
    "    else:\n",
    "        # If no rows need tools, set these metrics to None\n",
    "        performance_metrics['model_called_correct_tool'] = None\n",
    "        performance_metrics['tool_call_success'] = None\n",
    "    \n",
    "    # Rename columns for clarity\n",
    "    performance_metrics.columns = [\n",
    "        'Tool Called When Needed (%)',\n",
    "        'Correct Tool Called (%)', \n",
    "        'Tool Call Success (%)'\n",
    "    ]\n",
    "    \n",
    "    # Convert to percentages\n",
    "    performance_metrics = performance_metrics * 100\n",
    "    \n",
    "    return performance_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd4e2c56-8c9f-48a1-be54-084bd8417d6f",
   "metadata": {},
   "source": [
    "#### global results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6d067d6a-f69b-47f5-9b35-e668b1ec31a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_summary = compare_model_performance(results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "617315a5-49ad-48b3-b8f8-4f309d8d42e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tool Called When Needed (%)</th>\n",
       "      <th>Correct Tool Called (%)</th>\n",
       "      <th>Tool Call Success (%)</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>model_name</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>google/gemma-3-1b-it</th>\n",
       "      <td>90.00</td>\n",
       "      <td>35.0</td>\n",
       "      <td>50.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>shawhin/gemma-3-1b-tool-use</th>\n",
       "      <td>93.33</td>\n",
       "      <td>52.5</td>\n",
       "      <td>67.5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             Tool Called When Needed (%)  \\\n",
       "model_name                                                 \n",
       "google/gemma-3-1b-it                               90.00   \n",
       "shawhin/gemma-3-1b-tool-use                        93.33   \n",
       "\n",
       "                             Correct Tool Called (%)  Tool Call Success (%)  \n",
       "model_name                                                                   \n",
       "google/gemma-3-1b-it                            35.0                   50.0  \n",
       "shawhin/gemma-3-1b-tool-use                     52.5                   67.5  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "601d8c83-46df-4e57-8504-98c22eb6e6fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# write results to file\n",
    "eval_summary.to_csv('data/eval_summary.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deeb6687-2157-47a5-b722-ff44fb50fe6d",
   "metadata": {},
   "source": [
    "#### results by query type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9f2ba25d-8ea2-44a1-abb8-3e4d3cc8c344",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_summary_by_type = compare_model_performance_by_column(results_df, 'query_type')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1776088f-530e-4d61-aa3d-a704646231a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>Tool Called When Needed (%)</th>\n",
       "      <th>Correct Tool Called (%)</th>\n",
       "      <th>Tool Call Success (%)</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>model_name</th>\n",
       "      <th>query_type</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">google/gemma-3-1b-it</th>\n",
       "      <th>easy</th>\n",
       "      <td>100.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>65.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hard</th>\n",
       "      <td>100.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>35.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>no_tool</th>\n",
       "      <td>70.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">shawhin/gemma-3-1b-tool-use</th>\n",
       "      <th>easy</th>\n",
       "      <td>95.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>80.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hard</th>\n",
       "      <td>85.0</td>\n",
       "      <td>45.0</td>\n",
       "      <td>55.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>no_tool</th>\n",
       "      <td>100.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        Tool Called When Needed (%)  \\\n",
       "model_name                  query_type                                \n",
       "google/gemma-3-1b-it        easy                              100.0   \n",
       "                            hard                              100.0   \n",
       "                            no_tool                            70.0   \n",
       "shawhin/gemma-3-1b-tool-use easy                               95.0   \n",
       "                            hard                               85.0   \n",
       "                            no_tool                           100.0   \n",
       "\n",
       "                                        Correct Tool Called (%)  \\\n",
       "model_name                  query_type                            \n",
       "google/gemma-3-1b-it        easy                           50.0   \n",
       "                            hard                           20.0   \n",
       "                            no_tool                         NaN   \n",
       "shawhin/gemma-3-1b-tool-use easy                           60.0   \n",
       "                            hard                           45.0   \n",
       "                            no_tool                         NaN   \n",
       "\n",
       "                                        Tool Call Success (%)  \n",
       "model_name                  query_type                         \n",
       "google/gemma-3-1b-it        easy                         65.0  \n",
       "                            hard                         35.0  \n",
       "                            no_tool                       NaN  \n",
       "shawhin/gemma-3-1b-tool-use easy                         80.0  \n",
       "                            hard                         55.0  \n",
       "                            no_tool                       NaN  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_summary_by_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54e2cdf8-de7f-428a-8704-20949fb585ae",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
